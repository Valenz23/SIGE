library(caret)
library(tidyverse)
library(funModeling)
library(pROC)
library(partykit)
library(rattle)
library(randomForest)
library(xgboost)
#' @param data Datos originales
#' @param predictionProb Predicciones
#' @param target_var Variable objetivo de predicción
#' @param positive_class Clase positiva de la predicción
#'
#' @return Lista con valores de resultado \code{$auc}, \code{$roc}
#'
#' @examples
#' rfModel <- train(Class ~ ., data = train, method = "rf", metric = "ROC", trControl = rfCtrl, tuneGrid = rfParametersGrid)
#' roc_res <- my_roc(data = validation, predict(rfModel, validation, type = "prob"), "Class", "Good")
my_roc <- function(data, predictionProb, target_var, positive_class) {
auc <- roc(data[[target_var]], predictionProb[[positive_class]], levels = unique(data[[target_var]]))
roc <- plot.roc(auc, ylim=c(0,1), type = "S" , print.thres = T, main=paste('AUC:', round(auc$auc[[1]], 2)))
return(list("auc" = auc, "roc" = roc))
}
# Usando "German credit card data"
# http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29
# Variable de clasificación: Class
data(GermanCredit)
data <- as_tibble(GermanCredit)
glimpse(data)
df_status(data)
ggplot(data) + geom_histogram(aes(x = Class, fill = Class), stat = 'count')
set.seed(0)
## Crear modelo de predicción usando rpart
# Particiones entrenamiento / test
trainIndex <- createDataPartition(data$Class, p = .75, list = FALSE, times = 1)
train <- data[ trainIndex, ]
val   <- data[-trainIndex, ]
# Entrenar modelo
rpartCtrl <- trainControl(
verboseIter = F,
classProbs = TRUE,
summaryFunction = twoClassSummary)
rpartParametersGrid <- expand.grid(
.cp = c(0.001, 0.01, 0.1, 0.5))
rpartModel <- train(
Class ~ .,
data = train,
method = "rpart",
metric = "ROC",
trControl = rpartCtrl,
tuneGrid = rpartParametersGrid)
print(rpartModel)
# Validacion
prediction     <- predict(rpartModel, val, type = "raw")
predictionProb <- predict(rpartModel, val, type = "prob")
auc <- roc(val$Class, predictionProb[["Good"]], levels = unique(val[["Class"]]))
roc_validation <- plot.roc(auc, ylim=c(0,1), type = "S" , print.thres = T, main=paste('Validation AUC:', round(auc$auc[[1]], 2)))
# Obtener valores de accuracy, precision, recall, f-score (usando confusionMatrix)
cm_val <- confusionMatrix(prediction, val[["Class"]], positive = "Good")
cm_val$table[c(2,1), c(2,1)] # invertir filas y columnas para ver primero la clase "Good"
# Obtener valores de accuracy, precision, recall, f-score (manualmente)
results <- cbind(val, prediction)
results <- results %>%
mutate(contingency = as.factor(
case_when(
Class == 'Good' & prediction == 'Good' ~ 'TP',
Class == 'Bad'  & prediction == 'Good' ~ 'FP',
Class == 'Bad'  & prediction == 'Bad'  ~ 'TN',
Class == 'Good' & prediction == 'Bad'  ~ 'FN')))
TP <- length(which(results$contingency == 'TP'))
TN <- length(which(results$contingency == 'TN'))
FP <- length(which(results$contingency == 'FP'))
FN <- length(which(results$contingency == 'FN'))
n  <- length(results$contingency)
table(results$contingency) # comprobar recuento de TP, TN, FP, FN
accuracy <- (TP + TN) / n
error <- (FP + FN) / n
precision   <- TP / (TP + FP)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
f_measure   <- (2 * TP) / (2 * TP + FP + FN)
# Otro modelo utilizando rpart con cross-validation
rpartCtrl_2 <- trainControl(
verboseIter = F,
classProbs = TRUE,
method = "repeatedcv",
number = 10,
repeats = 1,
summaryFunction = twoClassSummary)
rpartModel_2 <- train(Class ~ ., data = train, method = "rpart", metric = "ROC", trControl = rpartCtrl_2, tuneGrid = rpartParametersGrid)
print(rpartModel_2)
varImp(rpartModel_2)
dotPlot(varImp(rpartModel_2))
plot(rpartModel_2)
plot(rpartModel_2$finalModel)
text(rpartModel_2$finalModel)
partyModel_2 <- as.party(rpartModel_2$finalModel)
plot(partyModel_2, type = 'simple')
fancyRpartPlot(rpartModel_2$finalModel)
predictionProb <- predict(rpartModel_2, val, type = "prob")
roc_1 <- my_roc(val, predictionProb, "Class", "Good")
## Crear modelo de predicción usando rf
# Modelo básico, ajuste de manual de hiperparámetros (.mtry)
rfCtrl <- trainControl(verboseIter = F, classProbs = TRUE, method = "repeatedcv", number = 10, repeats = 1, summaryFunction = twoClassSummary)
rfParametersGrid <- expand.grid(.mtry = c(sqrt(ncol(train))))
rfModel <- train(Class ~ ., data = train, method = "rf", metric = "ROC", trControl = rfCtrl, tuneGrid = rfParametersGrid)
print(rfModel)
varImp(rfModel$finalModel)
varImpPlot(rfModel$finalModel)
my_roc(val, predict(rfModel, val, type = "prob"), "Class", "Good")
# Modelo básico, ajuste manual de hiperparámetros (.mtry) utilizando un intervalo
rfCtrl <- trainControl(verboseIter = F, classProbs = TRUE, method = "repeatedcv", number = 10, repeats = 1, summaryFunction = twoClassSummary)
rfParametersGrid <- expand.grid(.mtry = c(1:5))
rfModel <- train(Class ~ ., data = train, method = "rf", metric = "ROC", trControl = rfCtrl, tuneGrid = rfParametersGrid)
print(rfModel)
plot(rfModel)
plot(rfModel$finalModel)
my_roc(val, predict(rfModel, val, type = "prob"), "Class", "Good")
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(pROC)
library(funModeling)
library(rpart.plot)
library(Hmisc)
library(corrplot)
set.seed(0)
library(tidyverse)
data_raw <- read_csv('train.csv')
head(data_raw)
data <-
data_raw %>%
mutate(Survived = as.factor(ifelse(Survived == 1, 'Yes', 'No'))) %>%
mutate(Pclass = as.factor(Pclass)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
na.exclude()
# Parámetros
rpartCtrl <- trainControl(verboseIter = F, classProbs = TRUE, summaryFunction = twoClassSummary)
rpartParametersGrid <- expand.grid(.cp = c(0.01, 0.05))
# Conjuntos de entrenamiento y validación
trainIndex <- createDataPartition(data$Survived, p = .8, list = FALSE, times = 1)
train <- data[trainIndex, ]
# Entrenamiento del modelo
rpartModel <- train(Survived ~ .,
data = train,
method = "rpart",
metric = "ROC",
trControl = rpartCtrl,
tuneGrid = rpartParametersGrid)
# Visualización del modelo
rpart.plot(rpartModel$finalModel)
# Predicciones con clases
val        <- data[-trainIndex, ]
prediction <- predict(rpartModel, val, type = "raw")
# Predicciones con probabilidades
predictionValidationProb <- predict(rpartModel, val, type = "prob")
cm_train <- confusionMatrix(prediction, val[["Survived"]])
cm_train
auc <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
roc_validation <- plot.roc(auc,
ylim=c(0,1),
type = "S" ,
print.thres = TRUE,
main=paste('Validation AUC:', round(auc$auc[[1]], 2)))
rfModel <- train(Survived ~ ., data = train, method = "rf", metric = "ROC", trControl = rpartCtrl)
predictionValidationProb <- predict(rfModel, val, type = "prob")
auc <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
roc_validation <- plot.roc(auc,
ylim=c(0,1),
type = "S" ,
print.thres = TRUE,
main=paste('Validation AUC:', round(auc$auc[[1]], 2)))
varImp(rpartModel)
varImp(rfModel)
important_vars <- varImp(rfModel)$importance %>%
rownames_to_column() %>%
filter(Overall > 25) %>%
select(rowname)
# correlation_table(data, target='Survived')
data_num <-
data %>%
mutate_if(is.character, as.factor) %>%
mutate_if(is.factor, as.numeric)
cor(data_num)
# correlation_table(data_num, target='Survived')
rcorr(as.matrix(data_num))
corrplot(cor(data_num), type = "upper", diag = F, order = "hclust", tl.col = "black", tl.srt = 45)
heatmap(x = cor(data_num), symm = TRUE)
var_rank_info(data, "Survived")
data_reduced <-
data %>%
select(Survived, Sex, Fare, Age, Pclass, SibSp)
head(data_reduced)
train <- data_reduced[trainIndex, ]
val   <- data_reduced[-trainIndex, ]
# rpart
rpartModel_2 <- train(Survived ~ ., data = train, method = "rpart", metric = "ROC", trControl = rpartCtrl, tuneGrid = rpartParametersGrid)
auc_rpart_2 <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
auc_rpart_2
roc_rpart_2 <- plot.roc(auc_rpart_2, ylim=c(0,1), type = "S" , print.thres = T, main=paste('Validation AUC:', round(auc_rpart_2$auc[[1]], 2)))
# rf
rfModel_2 <- train(Survived ~ ., data = train, method = "rf", metric = "ROC", trControl = rpartCtrl)
predictionValidationProb <- predict(rfModel_2, val, type = "prob")
auc_rf_2 <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
auc_rf_2
roc_rf_2 <- plot.roc(auc_rf_2, ylim=c(0,1), type = "S" , print.thres = T, main=paste('Validation AUC:', round(auc_rf_2$auc[[1]], 2)))
# comparacion
roc.test(roc_rf_2, roc_rpart_2)
plot.roc(auc_rf_2, type = "S", col="#1c61b6")
lines.roc(roc_rpart_2, type = "S", col="#008600")
train_num <-
data[trainIndex, ]  %>%
mutate_if(is.character, as.factor) %>%
mutate_if(is.factor, as.numeric)
val_num <-
data[-trainIndex, ]  %>%
mutate_if(is.character, as.factor) %>%
mutate_if(is.factor, as.numeric)
pca <- prcomp(train_num[,-1], scale=TRUE)
summary(pca)
library(ggbiplot)
ggbiplot(pca, groups = as.factor(train_num$Sex), ellipse = TRUE) +
scale_colour_manual(name="Sex", labels= c("Female", "Male"), values= c("orange", "lightblue"))
train_samples_proj <- predict(pca, train_num)
train_proj <- as_tibble(train_samples_proj[,1:4]) %>%
mutate(Survived = train$Survived)
# rpart con PCA
rpartModel_3 <- train(Survived ~ .,
data = train_proj,
method = "rpart",
metric = "ROC",
trControl = rpartCtrl,
tuneGrid = rpartParametersGrid)
rpartModel_3
rpart.plot(rpartModel_3$finalModel)
val_samples_proj <- predict(pca, val_num)
val_proj <- as_tibble(val_samples_proj[,1:4]) %>%
mutate(Survived = val$Survived)
predictionValidationProb <- predict(rpartModel_3, val_proj, type = "prob")
auc_rpart_3 <- roc(val_proj$Survived, predictionValidationProb[["Yes"]], levels = unique(val_proj[["Survived"]]))
auc_rpart_3
roc_rpart_3 <- plot.roc(auc_rpart_3, ylim=c(0,1), type = "S" , print.thres = T, main=paste('Validation AUC:', round(auc_rpart_3$auc[[1]], 2)))
